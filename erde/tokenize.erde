local config = require('erde.config')
local {
  DIGIT,
  HEX,
  STANDARD_ESCAPE_CHARS,
  SYMBOLS,
  TOKEN_TYPE,
  WORD_BODY,
  WORD_HEAD,
} = require('erde.constants')
local { get_source_alias } = require('erde.utils')

-- Foward declare
local tokenize_token

-- -----------------------------------------------------------------------------
-- State
-- -----------------------------------------------------------------------------

-- The source text to tokenize
local text = ''

-- The current character we are processing
local current_char = ''

-- The index of `current_char` in `text`
local current_char_index = 1

-- The line number of `current_char` in `text`
local current_line = 1

-- The name to use when referencing `text`. Used for error reporting.
local source_name = ''

-- -----------------------------------------------------------------------------
-- Helpers
-- -----------------------------------------------------------------------------

local function peek(n) {
  return text:sub(current_char_index, current_char_index + n - 1)
}

local function look_ahead(n) {
  return text:sub(current_char_index + n, current_char_index + n)
}

local function throw(message, line = current_line) {
  -- Use error level 0 since we already include `source_name`
  error("{ source_name }:{ line }: { message }", 0)
}

local function commit(token, line) {
  num_tokens = num_tokens + 1
  tokens[num_tokens] = token
  token_lines[num_tokens] = line || current_line
}

local function consume(n = 1) {
  local consumed = n == 1 && current_char || peek(n)
  current_char_index += n
  current_char = text:sub(current_char_index, current_char_index)
  return consumed
}

-- -----------------------------------------------------------------------------
-- Newline
-- -----------------------------------------------------------------------------

local function newline() {
  current_line += 1
  return consume() -- '\n'
}

-- -----------------------------------------------------------------------------
-- Numbers
-- -----------------------------------------------------------------------------

local function tokenize_binary() {
  consume(2) -- 0[bB]

  if current_char != '0' && current_char != '1' {
    throw('malformed binary')
  }

  local value = 0

  repeat {
    value = 2 * value + tonumber(consume())
  } until current_char != '0' && current_char != '1'

  return {
    type = TOKEN_TYPE.NUMBER,
    line = current_line,
    value = tostring(value),
  }
}

local function tokenize_decimal() {
  local value = ''

  while DIGIT[current_char] {
    value ..= consume()
  }

  if current_char == '.' && DIGIT[look_ahead(1)] {
    value ..= consume(2)

    while DIGIT[current_char] {
      value ..= consume()
    }
  }

  if current_char == 'e' || current_char == 'E' {
    value ..= consume()

    if current_char == '+' || current_char == '-' {
      value ..= consume()
    }

    if !DIGIT[current_char] {
      throw('missing exponent value')
    }

    while DIGIT[current_char] {
      value ..= consume()
    }
  }

  return {
    type = TOKEN_TYPE.NUMBER,
    line = current_line,
    value = value,
  }
}

local function tokenize_hex() {
  consume(2) -- 0[xX]

  if !HEX[current_char] && !(current_char == '.' && HEX[look_ahead(1)]) {
    throw('malformed hex')
  }

  local value = 0

  while HEX[current_char] {
    value = 16 * value + tonumber(consume(), 16)
  }

  if current_char == '.' && HEX[look_ahead(1)] {
    consume()

    local counter = 1

    repeat {
      value += tonumber(consume(), 16) / (16 ^ counter)
      counter += 1
    } until !HEX[current_char]
  }

  if current_char == 'p' || current_char == 'P' {
    consume()

    local exponent, sign = 0, 1

    if current_char == '+' || current_char == '-' {
      sign *= tonumber(consume() .. '1')
    }

    if !DIGIT[current_char] {
      throw('missing exponent value')
    }

    repeat {
      exponent = 10 * exponent + tonumber(consume())
    } until !DIGIT[current_char]

    value = value * 2 ^ (sign * exponent)
  }

  return {
    type = TOKEN_TYPE.NUMBER,
    line = current_line,
    value = tostring(value),
  }
}

-- -----------------------------------------------------------------------------
-- Strings
-- -----------------------------------------------------------------------------

local function escape_sequence() {
  if STANDARD_ESCAPE_CHARS[current_char] {
    return consume()
  } elseif DIGIT[current_char] {
    return consume()
  } elseif current_char == 'z' {
    if config.lua_target == '5.1' || config.lua_target == '5.1+' {
      throw('escape sequence \\z not compatible w/ lua targets 5.1, 5.1+')
    }

    return consume()
  } elseif current_char == 'x' {
    if config.lua_target == '5.1' || config.lua_target == '5.1+' {
      throw('escape sequence \\xXX not compatible w/ lua targets 5.1, 5.1+')
    }

    if !HEX[look_ahead(1)] || !HEX[look_ahead(2)] {
      throw('escape sequence \\xXX must use exactly 2 hex characters')
    }

    return consume(3)
  } elseif current_char == 'u' {
    if config.lua_target == '5.1' || config.lua_target == '5.1+' || config.lua_target == '5.2' || config.lua_target == '5.2+' {
      throw('escape sequence \\u{XXX} not compatible w/ lua targets 5.1, 5.1+, 5.2, 5.2+')
    }

    local sequence = consume() -- u

    if current_char != '{' {
      throw('missing { in escape sequence \\u{XXX}')
    }

    sequence ..= consume()

    if !HEX[current_char] {
      throw('missing hex in escape sequence \\u{XXX}')
    }

    while HEX[current_char] {
      sequence ..= consume()
    }

    if current_char != '}' {
      throw('missing } in escape sequence \\u{XXX}')
    }

    return sequence .. consume()
  } else {
    throw("invalid escape sequence \\{ current_char }")
  }
}

local function tokenize_interpolation() {
  consume() -- '{'

  local token = { type = TOKEN_TYPE.INTERPOLATION }
  local interpolation_line = current_line
  local brace_depth = 0 -- Keep track of brace depth in case of nested braces

  while current_char != '}' || brace_depth > 0 {
    if current_char == '{' {
      brace_depth += 1

      table.insert(token, {
        type = TOKEN_TYPE.SYMBOL,
        value = consume(),
      })
    } elseif current_char == '}' {
      brace_depth -= 1

      table.insert(token, {
        type = TOKEN_TYPE.SYMBOL,
        value = consume(),
      })
    } elseif current_char == '' {
      throw('unterminated interpolation', interpolation_line)
    } else {
      table.insert(token, tokenize_token())
    }
  }

  consume() -- '}'

  return token
}

local function tokenize_single_quote_string() {
  consume() -- "'"

  local content = ''

  while current_char != "'" {
    if current_char == '' || current_char == '\n' {
      throw('unterminated string')
    } elseif current_char == '\\' {
      content ..= consume() .. escape_sequence()
    } else {
      content ..= consume()
    }
  }

  consume() -- "'"

  return {
    type = TOKEN_TYPE.SINGLE_QUOTE_STRING,
    line = current_line,
    value = content,
  }
}

local function tokenize_double_quote_string() {
  consume() -- '"'

  local token = {
    type = TOKEN_TYPE.DOUBLE_QUOTE_STRING,
    line = current_line,
  }

  local content = ''

  while current_char != '"' {
    if current_char == '' || current_char == '\n' {
      throw('unterminated string')
    } elseif current_char == '\\' {
      consume()

      if current_char == '{' || current_char == '}' {
        content ..= consume()
      } else {
        content ..= '\\' .. escape_sequence()
      }
    } elseif current_char == '{' {
      if content != '' {
        table.insert(token, {
          type = TOKEN_TYPE.STRING_CONTENT,
          value = content,
        })

        content = ''
      }

      table.insert(token, tokenize_interpolation())
    } else {
      content ..= consume()
    }
  }

  consume() -- '"'

  if content != '' {
    table.insert(token, {
      type = TOKEN_TYPE.STRING_CONTENT,
      value = content,
    })
  }

  return token
}

local function tokenize_block_string() {
  consume() -- '['

  local token = {
    type = TOKEN_TYPE.BLOCK_STRING,
    line = current_line,
    equals = '',
  }

  while current_char == '=' {
    token.equals ..= consume()
  }

  if current_char != '[' {
    throw('unterminated block string opening', current_line)
  }

  consume() -- '['

  local close_quote = ']' .. token.equals .. ']'
  local close_quote_len = #close_quote
  local content = ''

  -- Check `current_char ~= ']'` first as slight optimization
  while current_char != ']' || peek(close_quote_len) != close_quote {
    if current_char == '' {
      throw('unterminated block string', token.line)
    } elseif current_char == '\n' {
      content ..= newline()
    } elseif current_char == '\\' {
      consume()

      if current_char == '{' || current_char == '}' {
        content ..= consume()
      } else {
        content ..= '\\' .. escape_sequence()
      }
    } elseif current_char == '{' {
      if content != '' {
        table.insert(token, {
          type = TOKEN_TYPE.STRING_CONTENT,
          value = content,
        })

        content = ''
      }

      table.insert(token, tokenize_interpolation())
    } else {
      content ..= consume()
    }
  }

  consume(close_quote_len)

  if content != '' {
    table.insert(token, {
      type = TOKEN_TYPE.STRING_CONTENT,
      value = content,
    })
  }

  return token
}

-- -----------------------------------------------------------------------------
-- Misc Tokenizers
-- -----------------------------------------------------------------------------

local function tokenize_word() {
  local word = consume()

  while WORD_BODY[current_char] {
    word ..= consume()
  }

  return {
    type = TOKEN_TYPE.WORD,
    line = current_line,
    value = word,
  }
}

local function tokenize_comment() {
  consume(2) -- '--'

  local is_block_comment, equals = false, ''

  if current_char == '[' {
    consume()

    while current_char == '=' {
      equals ..= consume()
    }

    if current_char == '[' {
      consume()
      is_block_comment = true
    }
  }

  if !is_block_comment {
    while current_char != '' && current_char != '\n' {
      consume()
    }
  } else {
    local close_quote = ']' .. equals .. ']'
    local close_quote_len = #close_quote
    local comment_line = current_line

    -- Check `current_char ~= ']'` first as slight optimization
    while current_char != ']' || peek(close_quote_len) != close_quote {
      if current_char == '' {
        throw('unterminated comment', comment_line)
      } elseif current_char == '\n' {
        newline()
      } else {
        consume()
      }
    }

    consume(close_quote_len)
  }
}

function tokenize_token() {
  if current_char == '\n' {
    newline()
  } elseif current_char == ' ' || current_char == '\t' {
    consume()
  } elseif WORD_HEAD[current_char] {
    return tokenize_word()
  } elseif current_char == "'" {
    return tokenize_single_quote_string()
  } elseif current_char == '"' {
    return tokenize_double_quote_string()
  } else {
    local peek_two = peek(2)

    if peek_two == '--' {
      tokenize_comment()
    } elseif peek_two == '0x' || peek_two == '0X' {
      return tokenize_hex()
    } elseif peek_two == '0b' || peek_two == '0B' {
      return tokenize_binary()
    } elseif DIGIT[current_char] || (current_char == '.' && DIGIT[look_ahead(1)]) {
      return tokenize_decimal()
    } elseif peek_two == '[[' || peek_two == '[=' {
      return tokenize_block_string()
    } elseif SYMBOLS[peek(3)] {
      return {
        type = TOKEN_TYPE.SYMBOL,
        line = current_line,
        value = consume(3),
      }
    } elseif SYMBOLS[peek_two] {
      return {
        type = TOKEN_TYPE.SYMBOL,
        line = current_line,
        value = consume(2),
      }
    } else {
      return {
        type = TOKEN_TYPE.SYMBOL,
        line = current_line,
        value = consume(1),
      }
    }
  }
}

-- -----------------------------------------------------------------------------
-- Main
-- -----------------------------------------------------------------------------

return (new_text, new_source_name) -> {
  text = new_text
  current_char = text:sub(1, 1)
  current_char_index = 1
  current_line = 1
  source_name = new_source_name || get_source_alias(text)

  local tokens = {}

  if peek(2) == '#!' {
    local shebang = consume(2)

    while current_char != '' && current_char != '\n' {
      shebang ..= consume()
    }

    table.insert(tokens, {
      type = TOKEN_TYPE.SHEBANG,
      value = shebang,
    })
  }

  while current_char != '' {
    local token = tokenize_token()

    if token != nil {
      table.insert(tokens, token)
    }
  }

  return tokens
}
