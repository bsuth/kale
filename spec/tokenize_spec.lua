local tokenize = require('erde.tokenize')
local C = require('erde.constants')

local function assertToken(token)
  local tokens = tokenize(token)
  assert.are.equal(token, tokens[1])
end

local function assertTokens(text, expectedTokens)
  local tokens = tokenize(text)
  assert.subtable(expectedTokens, tokens)
end

local function assertTokenLines(text, expectedTokenLines)
  local tokens, tokenLines = tokenize(text)
  assert.subtable(expectedTokenLines, tokenLines)
end

describe('tokenize #5.1+', function()
  spec('symbols', function()
    for symbol in pairs(C.SYMBOLS) do
      assertToken(symbol)
    end
  end)

  describe('words', function()
    spec('word head', function()
      assertToken('lua')
      assertToken('Erde')
      assertToken('_test')

      assert.has_error(function()
        tokenize('1word')
      end)
    end)

    spec('word body', function()
      assertToken('aa1B_')
      assertToken('_aa1B')
      assertTokens('a-', { 'a', '-' })
    end)
  end)

  describe('numbers', function()
    describe('hex', function()
      spec('integer', function()
        assertToken('0x123456789')
        assertToken('0xabcdef')
        assertToken('0xABCDEF')
        assertToken('0xa1B2')
        assertToken('0x1')
        assertToken('0X1')

        assert.has_error(function()
          tokenize('1x3')
        end)
        assert.has_error(function()
          tokenize('0x')
        end)
        assert.has_error(function()
          tokenize('0xg')
        end)
      end)

      spec('float', function()
        assertToken('0x.1')
        assertToken('0xd.a')
        assertToken('0xfp1')
        assertToken('0xfP1')
        assertToken('0xfp+1')
        assertToken('0xfp-1')

        assert.has_error(function()
          tokenize('0x.')
        end)
        assert.has_error(function()
          tokenize('0x.p1')
        end)
        assert.has_error(function()
          tokenize('0xfp+')
        end)
        assert.has_error(function()
          tokenize('0xfp-')
        end)
        assert.has_error(function()
          tokenize('0xfpa')
        end)
      end)
    end)

    describe('decimal', function()
      spec('integer', function()
        assertToken('9')
        assertToken('43')
      end)

      spec('float', function()
        assertToken('.34')
        assertToken('0.3')
        assertToken('1e2')
        assertToken('1E2')
        assertToken('3e+2')
        assertToken('3e-2')
        assertToken('1.23e29')

        assert.has_error(function()
          tokenize('4.')
        end)
        assert.has_error(function()
          tokenize('0.e1')
        end)
        assert.has_error(function()
          tokenize('9e')
        end)
        assert.has_error(function()
          tokenize('9e+')
        end)
        assert.has_error(function()
          tokenize('9e-')
        end)
      end)
    end)
  end)

  describe('strings', function()
    spec('single quote', function()
      assert.are.equal(3, #tokenize("''"))
      assertTokens("'hello'", { "'", 'hello', "'" })
      assertTokens("'a\\nb'", { "'", 'a\\nb', "'" })
      assertTokens("'\\\\'", { "'", '\\\\', "'" })

      assert.has_error(function()
        tokenize("'hello")
      end)
      assert.has_error(function()
        tokenize("'hello\nworld'")
      end)
    end)

    spec('double quote', function()
      assert.are.equal(2, #tokenize('""'))
      assertTokens('"hello"', { '"', 'hello', '"' })
      assertTokens('"hello\\nworld"', { '"', 'hello\\nworld', '"' })
      assertTokens('"\\\\"', { '"', '\\\\', '"' })

      assert.has_error(function()
        tokenize('"hello')
      end)
      assert.has_error(function()
        tokenize('"hello\nworld"')
      end)
    end)

    spec('long string', function()
      assertTokens('[[ a b ]]', { '[[', ' a b ', ']]' })
      assertTokens('[[a\nb]]', { '[[', 'a\nb', ']]' })
      assertTokens('[=[a[[b]=]', { '[=[', 'a[[b', ']=]' })

      assert.has_error(function()
        tokenize('[[hello world')
      end)
      assert.has_error(function()
        tokenize('[=hello world')
      end)
    end)

    spec('interpolation', function()
      assertTokens("'a{bc}d'", { "'", 'a{bc}d', "'" })
      assertTokens('"a{bc}d"', { '"', 'a', '{', 'bc', '}', 'd', '"' })
      assertTokens('[[a{bc}d]]', { '[[', 'a', '{', 'bc', '}', 'd', ']]' })

      assertTokens('"a{ bc  }d"', { '"', 'a', '{', 'bc', '}', 'd', '"' })

      assertTokens('"a\\{bc}d"', { '"', 'a{bc}d', '"' })
      assertTokens('[[a\\{bc}d]]', { '[[', 'a{bc}d', ']]' })

      assert.has_error(function()
        tokenize('"hello world {2"')
      end)
      assert.has_error(function()
        tokenize('"hello {2 world"')
      end)
    end)
  end)

  describe('comments', function()
    spec('short comment', function()
      assert.are.equal(0, #tokenize('--hello world'))
      assert.are.equal(0, #tokenize('-- hello world'))
      assert.subtable({ 'world' }, tokenize('--hello\nworld'))
    end)
    spec('long comment', function()
      assert.are.equal(0, #tokenize('--[[ hello world ]] '))
      assert.are.equal(0, #tokenize('--[[hello\nworld]] '))
      assert.are.equal(0, #tokenize('--[[ hello world ]] '))
      assert.are.equal(0, #tokenize('--[=[hello ]]]=] '))
      assert.are.equal(3, #tokenize('x + --[[hi]] 4'))
    end)
  end)

  describe('tokenInfo', function()
    spec('tokenInfo', function()
      assertTokenLines('a\nb', { 1, 2 })
      assertTokenLines('hello world\ngoodbye world', { 1, 1, 2, 2 })
    end)
  end)
end)
